{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0facc2-d3ea-428f-943c-61f96665bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def build_search_url(keywords):\n",
    "    # Process each keyword, replacing spaces with +, and surrounding phrases with %22\n",
    "    processed_keywords = []\n",
    "    for keyword in keywords:\n",
    "        if \" \" in keyword:  # For multi-word phrases\n",
    "            processed_keywords.append('%22' + keyword.replace(\" \", \"+\") + '%22')\n",
    "        else:\n",
    "            processed_keywords.append(keyword.replace(\" \", \"+\"))  # For single words, just replace spaces with +\n",
    "    \n",
    "    # Join the processed keywords with '+' to create a valid URL query string\n",
    "    query = \"+\".join(processed_keywords)\n",
    "    \n",
    "    # Construct the search URL dynamically\n",
    "    search_url = f\"https://link.springer.com/search?new-search=true&query={query}&content-type=article&dateFrom=&dateTo=&language=En&taxonomy=%22Artificial+Intelligence%22&facet-sub-discipline=%22Artificial+Intelligence%22&facet-sub-discipline=%22Computer+Science%2C+general%22&sortBy=relevance\"\n",
    "    \n",
    "    return search_url\n",
    "\n",
    "\n",
    "\n",
    "def create_empty_table(db_name, table_name):\n",
    "    # Connect to SQLite database (it will create the database file if it doesn't exist)\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # SQL query to create a table with the specified columns, with 'frequency' as the first column\n",
    "    create_table_query = f'''\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            frequency INTEGER DEFAULT 1,  -- New column to track frequency of duplicates (now the first column)\n",
    "            journal_name TEXT,\n",
    "            journal_link TEXT,\n",
    "            paper_title TEXT,\n",
    "            paper_link TEXT,\n",
    "            publication_date TEXT,\n",
    "            publishing_model TEXT,\n",
    "            journal_impact_factor TEXT,\n",
    "            five_year_journal_impact_factor TEXT,\n",
    "            submission_to_first_decision TEXT,\n",
    "            downloads TEXT\n",
    "        )\n",
    "    '''\n",
    "\n",
    "    # Execute the query to create the table\n",
    "    cursor.execute(create_table_query)\n",
    "    \n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Table '{table_name}' created successfully in database '{db_name}' with 'frequency' as the first column.\")\n",
    "\n",
    "\n",
    "def update_frequency(db_name, table_name):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "\n",
    "    # Load the existing data from the SQLite database into a DataFrame\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "    # Check if the 'frequency' column exists in the table, if not, create it\n",
    "    if 'frequency' not in df.columns:\n",
    "        df['frequency'] = 1  # Add 'frequency' column with default value of 1\n",
    "    else:\n",
    "        print(\"Frequency column already exists.\")\n",
    "\n",
    "    # Group by 'journal_name' and 'journal_link' to count duplicates\n",
    "    df['frequency'] = df.groupby(['journal_name', 'journal_link'])['journal_name'].transform('count')\n",
    "\n",
    "    # Drop duplicates based on 'journal_name' and 'journal_link', keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['journal_name', 'journal_link'], keep='first')\n",
    "\n",
    "    # Replace the existing table with the updated DataFrame\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "    print(f\"Frequency updated and duplicates removed in table '{table_name}'.\")\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def add_select_column(db_name, table_name):\n",
    "    # Step 1: Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "\n",
    "    # Step 2: Load the existing data from the specified table into a DataFrame\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "    # Step 3: Create the new 'select' column with default value 0\n",
    "    df.insert(0, 'select', 0)  # Insert 'select' as the first column, with default value 0\n",
    "\n",
    "    # Step 4: Replace the existing table with the updated DataFrame\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "    # Step 5: Close the connection to the database\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Column 'select' added as the first column with default value 0 and table '{table_name}' updated in database '{db_name}'.\")\n",
    "\n",
    "\n",
    "def scrape_springer_journals(search_url, num_pages, db_name, table_name):\n",
    "    # Set up the WebDriver (Chrome in this case)\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # Connect to SQLite database (we assume the table is already created)\n",
    "    conn = sqlite3.connect(db_name)\n",
    "\n",
    "    # Retrieve existing journal links from the database\n",
    "    existing_links = pd.read_sql_query(f\"SELECT journal_link FROM {table_name}\", conn)['journal_link'].tolist()\n",
    "\n",
    "    # DataFrame to store results temporarily with updated column names (snake_case)\n",
    "    columns = ['journal_name', 'journal_link', 'paper_title', 'paper_link', 'publication_date', \n",
    "               'publishing_model', 'journal_impact_factor', 'five_year_journal_impact_factor', \n",
    "               'submission_to_first_decision', 'downloads']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Loop over the pages\n",
    "    for page in range(1, num_pages + 1):\n",
    "        # Format the URL for pagination\n",
    "        formatted_url = f\"{search_url}&page={page}\"\n",
    "        print(f\"Scraping page {page}: {formatted_url}\")\n",
    "        \n",
    "        # Open the URL in the browser\n",
    "        driver.get(formatted_url)\n",
    "        \n",
    "        # Wait for the page to load dynamically\n",
    "        time.sleep(5)  # Adjust if necessary based on your connection speed\n",
    "        \n",
    "        # Get the page source and parse it with BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Find all Sample Paper elements\n",
    "        SamplePaper_elements = soup.select('li.app-card-open')  # Update to match the correct list class\n",
    "        \n",
    "        # Loop over each SamplePaper element and extract information\n",
    "        for SamplePaper in SamplePaper_elements:\n",
    "            # Extract the paper title and link\n",
    "            SamplePaper_name_element = SamplePaper.select_one('.app-card-open__link')\n",
    "            SamplePaper_title = SamplePaper_name_element.get_text(strip=True) if SamplePaper_name_element else 'N/A'\n",
    "            SamplePaper_link = SamplePaper_name_element.get('href') if SamplePaper_name_element else 'N/A'\n",
    "            \n",
    "            # Append base URL if the link is relative\n",
    "            if SamplePaper_link and not SamplePaper_link.startswith(\"http\"):\n",
    "                SamplePaper_link = \"https://link.springer.com\" + SamplePaper_link\n",
    "            \n",
    "            # Extract the journal name and link\n",
    "            journal_name_element = SamplePaper.select_one('.u-color-inherit')\n",
    "            journal_name = journal_name_element.get_text(strip=True) if journal_name_element else 'Journal name not found'\n",
    "            journal_link = journal_name_element.get('href') if journal_name_element else 'N/A'\n",
    "            \n",
    "            # Append base URL if the journal link is relative\n",
    "            if journal_link and not journal_link.startswith(\"http\"):\n",
    "                journal_link = \"https://link.springer.com\" + journal_link\n",
    "\n",
    "            # Check if the journal link already exists in the database\n",
    "            if journal_link in existing_links:\n",
    "                print(f\"Skipping {journal_name} as it's already in the database.\")\n",
    "                continue  # Skip to the next journal if it's already in the database\n",
    "\n",
    "            # Extract the publication date\n",
    "            publication_date_element = SamplePaper.find('span', {'data-test': 'published'})\n",
    "            publication_date = publication_date_element.get_text(strip=True) if publication_date_element else 'Date not found'\n",
    "            publication_date = publication_date.split()[-1]\n",
    "            \n",
    "            # Create a row with extracted data and fill the rest with empty values\n",
    "            temp_df = pd.DataFrame({\n",
    "                'journal_name': [journal_name],\n",
    "                'journal_link': [journal_link],\n",
    "                'paper_title': [SamplePaper_title.replace('’', \"'\").replace('–', '-')],\n",
    "                'paper_link': [SamplePaper_link],\n",
    "                'publication_date': [publication_date],\n",
    "                'publishing_model': [\"\"],  # Initially empty\n",
    "                'journal_impact_factor': [\"\"],  # Initially empty\n",
    "                'five_year_journal_impact_factor': [\"\"],  # Initially empty\n",
    "                'submission_to_first_decision': [\"\"],  # Initially empty\n",
    "                'downloads': [\"\"],  # Initially empty\n",
    "            })\n",
    "            \n",
    "            # Concatenate the temporary DataFrame with the main DataFrame\n",
    "            df = pd.concat([df, temp_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"Remained {num_pages - page} pages\")\n",
    "\n",
    "        # Wait for a random time between 5 and 25 seconds before proceeding to the next page\n",
    "        random_sleep_time = random.randint(5, 25)\n",
    "        time.sleep(random_sleep_time)\n",
    "\n",
    "    # Close the browser when done\n",
    "    driver.quit()\n",
    "\n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "\n",
    "    # Insert the DataFrame into the existing SQLite table (append the data)\n",
    "    if not df.empty:\n",
    "        df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "        print(f\"New data added to table '{table_name}' in database '{db_name}'\")\n",
    "    else:\n",
    "        print(\"No new data to add to the database.\")\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Data saved to table '{table_name}' in database '{db_name}'\")\n",
    "\n",
    "\n",
    "\n",
    "def scrape_journal_details(db_name, table_name, row_limit=None):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "\n",
    "    # Load the existing data from the SQLite database into a DataFrame\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "    # Replace empty strings with NaN (which is treated as null)\n",
    "    df.replace(\"\", None, inplace=True)\n",
    "\n",
    "    # Filter the DataFrame to get only rows where the following columns are missing or empty\n",
    "    missing_columns = ['publishing_model', 'journal_impact_factor', 'five_year_journal_impact_factor', 'submission_to_first_decision', 'downloads']\n",
    "    df_filtered = df[df[missing_columns].isnull().any(axis=1)]  # Rows with missing information or empty strings\n",
    "\n",
    "    # Apply row_limit only if it is set, otherwise process all rows\n",
    "    if row_limit:\n",
    "        df_filtered = df_filtered.head(row_limit)\n",
    "\n",
    "    # Set up the WebDriver (Chrome in this case)\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    total_rows = len(df_filtered)  # Get the total number of filtered rows (journals) in the DataFrame\n",
    "\n",
    "    # Loop over each journal row in the filtered DataFrame\n",
    "    for index, row in df_filtered.iterrows():\n",
    "        remaining_journals = total_rows - (index + 1)  # Calculate remaining journals\n",
    "\n",
    "        journal_name = row['journal_name']\n",
    "        journal_link = row['journal_link']\n",
    "\n",
    "        # Open the journal link\n",
    "        driver.get(journal_link)\n",
    "\n",
    "        # Random delay between 5-25 seconds\n",
    "        random_sleep_time = random.randint(5, 25)\n",
    "        print(f\"Waiting for {random_sleep_time} seconds before scraping journal page...\")\n",
    "        time.sleep(random_sleep_time)\n",
    "\n",
    "        # Get the page source and parse it with BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Extract publishing model\n",
    "        publishing_model = \"\"\n",
    "        publishing_model_element = soup.select_one('.app-journal-masthead__publishing-model-link')\n",
    "        if publishing_model_element:\n",
    "            publishing_model = publishing_model_element.get_text(strip=True)\n",
    "\n",
    "        # Extract the journal metrics (impact factor, 5-year impact factor, etc.)\n",
    "        impact_factor = \"\"\n",
    "        five_year_impact_factor = \"\"\n",
    "        submission_to_decision = \"\"\n",
    "        downloads = \"\"\n",
    "\n",
    "        metrics_section = soup.select_one('.app-journal-overview__metrics')\n",
    "\n",
    "        if metrics_section:\n",
    "            # Extract specific metrics\n",
    "            impact_factor_element = metrics_section.find('dd', {'data-test': 'impact-factor-value'})\n",
    "            if impact_factor_element:\n",
    "                impact_factor = impact_factor_element.get_text(strip=True)\n",
    "\n",
    "            five_year_impact_factor_element = metrics_section.find('dd', {'data-test': 'five-year-impact-factor-value'})\n",
    "            if five_year_impact_factor_element:\n",
    "                five_year_impact_factor = five_year_impact_factor_element.get_text(strip=True)\n",
    "\n",
    "            submission_to_decision_element = metrics_section.find('dd', {'data-test': 'metrics-speed-value'})\n",
    "            if submission_to_decision_element:\n",
    "                submission_to_decision = submission_to_decision_element.get_text(strip=True)\n",
    "\n",
    "            downloads_element = metrics_section.find('dd', {'data-test': 'metrics-downloads-value'})\n",
    "            if downloads_element:\n",
    "                downloads = downloads_element.get_text(strip=True)\n",
    "\n",
    "        # Ensure there's data to update\n",
    "        if publishing_model or impact_factor or five_year_impact_factor or submission_to_decision or downloads:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f'''\n",
    "                UPDATE {table_name}\n",
    "                SET publishing_model = ?, journal_impact_factor = ?, five_year_journal_impact_factor = ?, submission_to_first_decision = ?, downloads = ?\n",
    "                WHERE journal_link = ?\n",
    "            ''', (publishing_model, impact_factor, five_year_impact_factor, submission_to_decision, downloads, journal_link))\n",
    "            conn.commit()\n",
    "\n",
    "        print(f\"Updated details for {journal_name} (Link: {journal_link}).\")\n",
    "\n",
    "    # Close the browser and the database connection\n",
    "    driver.quit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Data updated in {db_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4834bb0-f988-4c35-8f92-f68af4676e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://link.springer.com/search?new-search=true&query=energy+%22deep+learning%22&content-type=article&dateFrom=&dateTo=&language=En&taxonomy=%22Artificial+Intelligence%22&facet-sub-discipline=%22Artificial+Intelligence%22&facet-sub-discipline=%22Computer+Science%2C+general%22&sortBy=relevance\n",
      "Column 'select' added as the first column with default value 0 and table 'energy_deeplearning' updated in database 'springer.db'.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "keywords = [\"energy\", \"deep learning\"]  # Example with a single word and multi-word phrases\n",
    "search_url = build_search_url(keywords)  # Assuming you have the build_search_url function\n",
    "print(search_url)\n",
    "\n",
    "table_name = '_'.join([item.replace(' ', '') for item in keywords])\n",
    "\n",
    "db_name = 'springer.db'\n",
    "\n",
    "# Create the table\n",
    "create_empty_table(db_name, table_name)\n",
    "\n",
    "# Run the scraper\n",
    "scrape_springer_journals(search_url=search_url, num_pages=2, db_name=db_name, table_name=table_name)\n",
    "\n",
    "# Update the frequency for repeated entries\n",
    "update_frequency(db_name, table_name)\n",
    "\n",
    "add_select_column(db_name, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9091a62-c4cb-49ee-ac01-734cc22cfd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 21 seconds before scraping journal page...\n",
      "Updated details for Neural Computing and Applications (Link: https://link.springer.com/journal/521).\n",
      "Waiting for 8 seconds before scraping journal page...\n",
      "Updated details for SN Computer Science (Link: https://link.springer.com/journal/42979).\n",
      "Waiting for 11 seconds before scraping journal page...\n",
      "Updated details for Artificial Intelligence Review (Link: https://link.springer.com/journal/10462).\n",
      "Waiting for 25 seconds before scraping journal page...\n",
      "Updated details for Applied Intelligence (Link: https://link.springer.com/journal/10489).\n",
      "Waiting for 11 seconds before scraping journal page...\n",
      "Updated details for Telecommunication Systems (Link: https://link.springer.com/journal/11235).\n",
      "Waiting for 13 seconds before scraping journal page...\n",
      "Updated details for Computing (Link: https://link.springer.com/journal/607).\n",
      "Waiting for 18 seconds before scraping journal page...\n",
      "Updated details for The Journal of Supercomputing (Link: https://link.springer.com/journal/11227).\n",
      "Waiting for 24 seconds before scraping journal page...\n",
      "Updated details for International Journal of Computer Assisted Radiology and Surgery (Link: https://link.springer.com/journal/11548).\n",
      "Waiting for 24 seconds before scraping journal page...\n",
      "Updated details for Fuzzy Optimization and Decision Making (Link: https://link.springer.com/journal/10700).\n",
      "Waiting for 18 seconds before scraping journal page...\n",
      "Updated details for Nature Computational Science (Link: https://link.springer.com/journal/43588).\n",
      "Waiting for 25 seconds before scraping journal page...\n",
      "Updated details for International Journal of Information Technology (Link: https://link.springer.com/journal/41870).\n",
      "Waiting for 20 seconds before scraping journal page...\n",
      "Updated details for International Journal of Computational Intelligence Systems (Link: https://link.springer.com/journal/44196).\n",
      "Waiting for 25 seconds before scraping journal page...\n",
      "Updated details for International Journal of Data Science and Analytics (Link: https://link.springer.com/journal/41060).\n",
      "Waiting for 13 seconds before scraping journal page...\n",
      "Updated details for Soft Computing (Link: https://link.springer.com/journal/500).\n",
      "Waiting for 10 seconds before scraping journal page...\n",
      "Updated details for Cognitive Computation (Link: https://link.springer.com/journal/12559).\n",
      "Waiting for 12 seconds before scraping journal page...\n",
      "Updated details for Evolutionary Intelligence (Link: https://link.springer.com/journal/12065).\n",
      "Waiting for 16 seconds before scraping journal page...\n",
      "Updated details for Journal of Reliable Intelligent Environments (Link: https://link.springer.com/journal/40860).\n",
      "Waiting for 19 seconds before scraping journal page...\n",
      "Updated details for International Journal of Machine Learning and Cybernetics (Link: https://link.springer.com/journal/13042).\n",
      "Waiting for 17 seconds before scraping journal page...\n",
      "Updated details for GeoInformatica (Link: https://link.springer.com/journal/10707).\n",
      "Waiting for 8 seconds before scraping journal page...\n",
      "Updated details for Computational Visual Media (Link: https://link.springer.com/journal/41095).\n",
      "Waiting for 18 seconds before scraping journal page...\n",
      "Updated details for International Journal of Fuzzy Systems (Link: https://link.springer.com/journal/40815).\n",
      "Waiting for 15 seconds before scraping journal page...\n",
      "Updated details for Discover Artificial Intelligence (Link: https://link.springer.com/journal/44163).\n",
      "Waiting for 8 seconds before scraping journal page...\n",
      "Updated details for Neural Processing Letters (Link: https://link.springer.com/journal/11063).\n",
      "Waiting for 21 seconds before scraping journal page...\n",
      "Updated details for Autonomous Intelligent Systems (Link: https://link.springer.com/journal/43684).\n",
      "Waiting for 15 seconds before scraping journal page...\n",
      "Updated details for Journal of Computer Science and Technology (Link: https://link.springer.com/journal/11390).\n",
      "Waiting for 22 seconds before scraping journal page...\n",
      "Updated details for Quantum Machine Intelligence (Link: https://link.springer.com/journal/42484).\n",
      "Waiting for 16 seconds before scraping journal page...\n",
      "Updated details for Journal of Shanghai Jiaotong University (Science) (Link: https://link.springer.com/journal/12204).\n",
      "Waiting for 7 seconds before scraping journal page...\n",
      "Updated details for Evolving Systems (Link: https://link.springer.com/journal/12530).\n",
      "Waiting for 23 seconds before scraping journal page...\n",
      "Updated details for International Journal of Speech Technology (Link: https://link.springer.com/journal/10772).\n",
      "Waiting for 19 seconds before scraping journal page...\n",
      "Updated details for The Visual Computer (Link: https://link.springer.com/journal/371).\n",
      "Waiting for 17 seconds before scraping journal page...\n",
      "Updated details for Journal of Intelligent Information Systems (Link: https://link.springer.com/journal/10844).\n",
      "Waiting for 6 seconds before scraping journal page...\n",
      "Updated details for Journal of Umm Al-Qura University for Engineering and Architecture (Link: https://link.springer.com/journal/43995).\n",
      "Waiting for 13 seconds before scraping journal page...\n",
      "Updated details for International Journal of Computer Vision (Link: https://link.springer.com/journal/11263).\n",
      "Waiting for 8 seconds before scraping journal page...\n",
      "Updated details for Minds and Machines (Link: https://link.springer.com/journal/11023).\n",
      "Waiting for 15 seconds before scraping journal page...\n",
      "Updated details for Frontiers of Computer Science (Link: https://link.springer.com/journal/11704).\n",
      "Waiting for 20 seconds before scraping journal page...\n",
      "Updated details for Artificial Life and Robotics (Link: https://link.springer.com/journal/10015).\n",
      "Waiting for 22 seconds before scraping journal page...\n",
      "Updated details for Machine Intelligence Research (Link: https://link.springer.com/journal/11633).\n",
      "Waiting for 13 seconds before scraping journal page...\n",
      "Updated details for Journal of Ambient Intelligence and Humanized Computing (Link: https://link.springer.com/journal/12652).\n",
      "Waiting for 17 seconds before scraping journal page...\n",
      "Updated details for Human-Centric Intelligent Systems (Link: https://link.springer.com/journal/44230).\n",
      "Waiting for 13 seconds before scraping journal page...\n",
      "Updated details for Journal of Intelligent & Robotic Systems (Link: https://link.springer.com/journal/10846).\n",
      "Waiting for 8 seconds before scraping journal page...\n",
      "Updated details for Frontiers of Information Technology & Electronic Engineering (Link: https://link.springer.com/journal/11714).\n",
      "Waiting for 5 seconds before scraping journal page...\n",
      "Updated details for Annals of Data Science (Link: https://link.springer.com/journal/40745).\n",
      "Waiting for 9 seconds before scraping journal page...\n",
      "Updated details for Intelligent Marine Technology and Systems (Link: https://link.springer.com/journal/44295).\n",
      "Waiting for 21 seconds before scraping journal page...\n",
      "Updated details for Cognitive Neurodynamics (Link: https://link.springer.com/journal/11571).\n",
      "Waiting for 24 seconds before scraping journal page...\n",
      "Updated details for Machine Learning (Link: https://link.springer.com/journal/10994).\n",
      "Waiting for 12 seconds before scraping journal page...\n",
      "Updated details for Journal of Membrane Computing (Link: https://link.springer.com/journal/41965).\n",
      "Waiting for 20 seconds before scraping journal page...\n",
      "Updated details for AI in Civil Engineering (Link: https://link.springer.com/journal/43503).\n",
      "Waiting for 7 seconds before scraping journal page...\n",
      "Updated details for Cybernetics and Systems Analysis (Link: https://link.springer.com/journal/10559).\n",
      "Data updated in springer.db\n"
     ]
    }
   ],
   "source": [
    "scrape_journal_details(db_name, table_name, row_limit=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
